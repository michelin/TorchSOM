{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsom.core import SOM\n",
    "from torchsom.visualization import SOMVisualizer, VisualizationConfig\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.exceptions import ConvergenceWarning, DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"../data/notebooks/boston_housing.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boston_df_scaled = boston_df\n",
    "scaler = StandardScaler()\n",
    "boston_df_scaled = pd.DataFrame(scaler.fit_transform(boston_df), columns=boston_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = boston_df_scaled.columns.to_list()[:-1]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Create a tensor from the boston df and separate the features and the target\n",
    "2. Randomly shuffle the data\n",
    "3. Split the data into training and testing sets\n",
    "\"\"\"\n",
    "boston_torch = torch.tensor(boston_df_scaled.to_numpy(dtype=np.float32), device=device)\n",
    "all_features, all_targets = boston_torch[:, :-1], boston_torch[:, -1]\n",
    "\n",
    "\n",
    "shuffled_indices = torch.randperm(len(all_features), device=device)\n",
    "all_features, all_targets = all_features[shuffled_indices], all_targets[shuffled_indices]\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_count = int(train_ratio * len(all_features))\n",
    "train_features, train_targets = all_features[:train_count], all_targets[:train_count]\n",
    "test_features, test_targets = all_features[train_count:], all_targets[train_count:]\n",
    "\n",
    "print(train_features.shape, test_features.shape)\n",
    "print(train_targets.shape, test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchSOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = SOM(\n",
    "    x=50,\n",
    "    y=30,\n",
    "    sigma=2.3,\n",
    "    learning_rate=0.95,\n",
    "    neighborhood_order=3,\n",
    "    epochs=125,\n",
    "    batch_size=16,\n",
    "    topology=\"hexagonal\",\n",
    "    distance_function=\"euclidean\",\n",
    "    neighborhood_function=\"gaussian\",\n",
    "    num_features=all_features.shape[1],\n",
    "    lr_decay_function=\"asymptotic_decay\",\n",
    "    sigma_decay_function=\"asymptotic_decay\",\n",
    "    initialization_mode=\"pca\",\n",
    "    device=device,\n",
    "    random_seed=random_seed,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.initialize_weights(\n",
    "    data=train_features,\n",
    "    mode=som.initialization_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QE, TE = som.fit(\n",
    "    data=train_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmus_map = som.build_map(\n",
    "    \"bmus_data\",\n",
    "    data=train_features,\n",
    "    # target=train_targets,\n",
    "    return_indices=True,\n",
    "    batch_size=train_features.shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SOMVisualizer(som=som, config=VisualizationConfig(save_format=\"pdf\"))\n",
    "save_path = f\"results/boston/{som.topology}\" # Set to None if you want a direct plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_training_errors(\n",
    "    quantization_errors=QE, \n",
    "    topographic_errors=TE, \n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_distance_map(\n",
    "    # fig_name=\"distance_map\",\n",
    "    save_path=save_path,\n",
    "    distance_metric=som.distance_fn_name,\n",
    "    neighborhood_order=som.neighborhood_order,\n",
    "    scaling=\"sum\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_hit_map(\n",
    "    # fig_name=\"hit_map\",\n",
    "    data=train_features,\n",
    "    save_path=save_path,\n",
    "    batch_size=train_features.shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_metric_map(\n",
    "    # fig_name=\"mean_metric_map\",\n",
    "    data=train_features,\n",
    "    target=train_targets,\n",
    "    reduction_parameter=\"mean\",\n",
    "    save_path=save_path,\n",
    "    bmus_data_map=bmus_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_metric_map(\n",
    "    # fig_name=\"std_metric_map\",\n",
    "    data=train_features,\n",
    "    target=train_targets,\n",
    "    reduction_parameter=\"std\",\n",
    "    save_path=save_path,\n",
    "    bmus_data_map=bmus_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_rank_map(\n",
    "    # fig_name=\"rank_map\",\n",
    "    bmus_data_map=bmus_map,\n",
    "    target=train_targets,\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_score_map(\n",
    "    # fig_name=\"score_map\",\n",
    "    bmus_data_map=bmus_map,\n",
    "    target=train_targets,\n",
    "    total_samples=train_features.shape[0],\n",
    "    save_path=save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_component_planes(\n",
    "    component_names=feature_names,\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Here, we do not add the testing samples in the SOM BMUs map.  \n",
    "In forecasting or process control, it is interesting to add overtime the new elements in the SOM and potentially to update/refit it with a certain frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for idx, (test_feature, test_target) in enumerate(zip(test_features, test_targets)):\n",
    "        \n",
    "    collected_features, collected_targets = som.collect_samples(\n",
    "        query_sample=test_feature,\n",
    "        historical_samples=train_features,\n",
    "        historical_outputs=train_targets,\n",
    "        min_buffer_threshold=125, # Collect 30 historical samples to train a model\n",
    "        bmus_idx_map=bmus_map,\n",
    "    )\n",
    "    \n",
    "    X = collected_features.cpu().numpy()\n",
    "    y = collected_targets.cpu().numpy().ravel()\n",
    "    test_feature_np = test_feature.cpu().numpy().reshape(1, -1)  \n",
    "    \n",
    "    reg = MLPRegressor(\n",
    "        hidden_layer_sizes=(8, 16, 16, 8),\n",
    "        max_iter=250,\n",
    "        learning_rate_init=0.005,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        batch_size='auto', \n",
    "        random_state=random_seed,\n",
    "        shuffle=True,\n",
    "        verbose=False,\n",
    "    ).fit(X, y)\n",
    "    \n",
    "    # plt.plot(reg.loss_curve_)\n",
    "    # plt.xlabel(\"Iteration\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"MLPRegressor Training Loss Curve\")\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    reg_prediction = reg.predict(test_feature_np)\n",
    "    predictions.append(reg_prediction[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(predictions)\n",
    "y_true = test_targets.cpu().numpy()     \n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred) \n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = root_mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = min(min(y_pred), min(y_true))\n",
    "max_val = max(max(y_pred), max(y_true))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    alpha=0.8,\n",
    "    color=\"blue\",\n",
    "    marker=\"o\",\n",
    "    edgecolor=\"black\",\n",
    "    s=50,\n",
    "    label=\"Predictions\",\n",
    ")\n",
    "plt.plot(\n",
    "    [min_val, max_val],\n",
    "    [min_val, max_val],\n",
    "    label=\"Perfect Prediction\",\n",
    "    color=\"red\",\n",
    "    alpha=0.8,\n",
    "    linewidth=2\n",
    ")\n",
    "plt.xlabel(\"Ground Truth Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torchsom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
